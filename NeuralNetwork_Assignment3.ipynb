{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxaaRfPKxnrC"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "********************************** Neural Network **********************************\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "Hidden Layers use ReLU activation function, output layer is softmax activation function\n",
        "and loss function is categorical cross entropy loss\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class layer_dense:\n",
        "  \n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    self.weights = 0.01*np.random.randn(n_inputs, n_neurons)/np.sqrt(n_inputs/2)\n",
        "    self.biases  = np.zeros((1, n_neurons))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    #remember inputs\n",
        "    self.inputs = inputs\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "  def backward(self, dvalues):\n",
        "    self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "    self.dbiases = np.sum(dvalues, axis = 0, keepdims = True)\n",
        "    self.dinputs = np.dot(dvalues, self.weights.T)\n",
        "  \n",
        "class Activation_ReLU:\n",
        "  def forward(self, inputs):\n",
        "    self.inputs = inputs\n",
        "    self.output = np.maximum(0, inputs)\n",
        "\n",
        "  def backward(self, dvalues):\n",
        "    self.dinputs = dvalues.copy()\n",
        "    self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "class Activation_Softmax:\n",
        "    def forward(self, inputs):\n",
        "      self.inputs = inputs\n",
        "      exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
        "      probabilities = exp_values/np.sum(exp_values, axis = 1, keepdims = True)\n",
        "      self.output = probabilities\n",
        "\n",
        "    def backward(self, dvalues):\n",
        "      self.dinputs = np.empty_like(dvalues)\n",
        "      for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
        "        single_output = single_output.reshape(-1, 1)\n",
        "        jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
        "        self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
        "\n",
        "class Loss:\n",
        "  def calculate(self, output, y):\n",
        "    sample_losses = self.forward(output, y)\n",
        "    data_loss = np.mean(sample_losses)\n",
        "    return data_loss\n",
        "  \n",
        "class Loss_CategoricalCrossEntropy(Loss):\n",
        "\n",
        "  def forward(self, y_pred, y_true):\n",
        "    samples = len(y_pred)\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "    if len(y_true.shape) == 1:\n",
        "      correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "    elif len(y_true.shape) != 1:\n",
        "      correct_confidences = np.sum(y_pred_clipped*y_true, axis = 1)\n",
        "\n",
        "    negative_log_likelihoods = -np.log(correct_confidences)\n",
        "    return negative_log_likelihoods\n",
        "\n",
        "  def backward(self, dvalues, y_true):\n",
        "    samples = len(dvalues)\n",
        "    labels = len(dvalues[0])\n",
        "    if len(y_true.shape) == 1:\n",
        "      y_true = np.eye(labels)[y_true]\n",
        "    self.dinputs = -y_true/dvalues\n",
        "    self.dinputs = self.dinputs/samples\n",
        "\n",
        "class Activation_Softmax_Loss_CategoricalCrossEntropy():\n",
        "\n",
        "  def __init__ (self):\n",
        "    self.activation = Activation_Softmax()\n",
        "    self.loss = Loss_CategoricalCrossEntropy()\n",
        "\n",
        "  def forward(self, inputs, y_true):\n",
        "    self.activation.forward(inputs)\n",
        "    self.output = self.activation.output\n",
        "    return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "  def backward(self, dvalues, y_true):\n",
        "    samples = len(dvalues)\n",
        "    if len(y_true.shape) != 1:\n",
        "      y_true = np.argmax(y_true, axis = 1)\n",
        "    self.dinputs = dvalues.copy()\n",
        "    self.dinputs[range(samples), y_true] -= 1\n",
        "    self.dinputs = self.dinputs/samples\n",
        "\n",
        "class Optimizer:\n",
        "\n",
        "  def __init__(self, learning_rate = 1.0):\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "  def update_params(self, layer):\n",
        "    layer.weights -= self.learning_rate*layer.dweights\n",
        "    layer.biases -= self.learning_rate*layer.dbiases\n",
        "\n",
        "\"\"\"\n",
        "********************************** Getting Images **********************************\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import cifar10\n",
        "import cv2\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "X = []  #stores 20 cat and dog images\n",
        "y = []  #stores the label associated with each image\n",
        "\n",
        "\"\"\"\n",
        "function that gets n images of a particular class\n",
        "and stores it in X and the labels in y\n",
        "\"\"\"\n",
        "def get_images(class_index, n):\n",
        "  count = 0;\n",
        "  i = 0\n",
        "  if class_index == 3:\n",
        "    label = 0\n",
        "  elif class_index == 5:\n",
        "    label = 1\n",
        "  while(count < n):\n",
        "    if(y_train[i] == class_index):\n",
        "      X.append(X_train[i])\n",
        "      y.append(label)\n",
        "      count += 1\n",
        "    i += 1\n",
        "\n",
        "\"\"\"\n",
        "In CIFAR10, cats are labeled 3 and dogs are labeled 5\n",
        "Here cats will belong to class 0 and dogs to class 1\n",
        "\"\"\"\n",
        "get_images(3, 10) #adding 10 images of cats to X\n",
        "get_images(5, 10) #adding 10 images of dogs to X\n",
        "\n",
        "#convert the images into grayscale\n",
        "X = np.array([cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) for image in X])\n",
        "\n",
        "\"\"\"\n",
        "function that displays n images belonging to class \"cls\"\n",
        "\"\"\"\n",
        "def displayImages(n, cls):\n",
        "  d = 0\n",
        "  if cls == 'dog':\n",
        "    d = 10\n",
        "  elif cls == 'cat':\n",
        "    d = 0\n",
        "  fig, *a = plt.subplots(1, n)\n",
        "  for i in range(n):\n",
        "    a[0][i].imshow(X[i + d], cmap = 'gray')\n",
        "    a[0][i].axis('off')\n",
        "\n",
        "displayImages(3, 'cat')\n",
        "displayImages(3, 'dog')\n",
        "plt.show()\n",
        "\n",
        "\"\"\"\n",
        "********************************** Preprocessing **********************************\n",
        "\"\"\"\n",
        "print(\"\\n\\n\\n\")\n",
        "\n",
        "#Converting the lists into numpy arrays\n",
        "#Coverting 32*32 image into 1024 array\n",
        "X = np.array(X, dtype = 'f').reshape(20, 32*32)\n",
        "y = np.array(y)\n",
        "\n",
        "#normalizing the data\n",
        "for i in range(X.shape[0]):\n",
        "  X[i] = (X[i] - np.mean(X[i]))/np.std(X[i])\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 2)\n",
        "\n",
        "\"\"\"\n",
        "********************************** Training **********************************\n",
        "\"\"\"\n",
        "\n",
        "layer1 = layer_dense(len(X_train[0]), 100)\n",
        "activation1 = Activation_ReLU()\n",
        "layer2 = layer_dense(100, 100)\n",
        "activation2 = Activation_ReLU()\n",
        "layer3 = layer_dense(100, 2)\n",
        "output_layer = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
        "\n",
        "optimizer = Optimizer(learning_rate = 0.1)    \n",
        "\n",
        "layer1.forward(X_train)                      \n",
        "activation1.forward(layer1.output)                        \n",
        "layer2.forward(layer1.output)\n",
        "activation2.forward(layer2.output) \n",
        "layer3.forward(activation2.output)                      \n",
        "loss = output_layer.forward(layer3.output, Y_train)\n",
        "\n",
        "fig, *a = plt.subplots(1, 2)\n",
        "a[0][0].hist(activation1.output.reshape(1, -1)[0], bins = 50)\n",
        "a[0][1].hist(activation2.output.reshape(1, -1)[0], bins = 50)\n",
        "plt.show()\n",
        "\n",
        "err = []\n",
        "acc = []\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "for epoch in range(500):\n",
        "  layer1.forward(X_train)                      \n",
        "  activation1.forward(layer1.output)                        \n",
        "  layer2.forward(layer1.output)\n",
        "  activation2.forward(layer2.output) \n",
        "  layer3.forward(activation2.output)                      \n",
        "  loss = output_layer.forward(layer3.output, Y_train)\n",
        "\n",
        "  predictions = np.argmax(output_layer.output, axis = 1)\n",
        "  if len(Y_train.shape) != 1:\n",
        "    Y_train = np.argmax(y, axis = 1)\n",
        "  accuracy = np.mean(predictions == Y_train)\n",
        "\n",
        "  if not epoch % 1:\n",
        "    #print('loss: ', loss)\n",
        "    err.append(loss)\n",
        "    acc.append(accuracy)\n",
        "\n",
        "  #Backward pass\n",
        "  output_layer.backward(output_layer.output, Y_train)\n",
        "  layer3.backward(output_layer.dinputs)\n",
        "  activation2.backward(layer3.dinputs)\n",
        "  layer2.backward(activation2.dinputs)\n",
        "  activation1.backward(layer2.dinputs)\n",
        "  layer1.backward(activation1.dinputs)\n",
        "  \n",
        "  optimizer.update_params(layer1)\n",
        "  optimizer.update_params(layer2)\n",
        "  optimizer.update_params(layer3)\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "#plt.figure()\n",
        "plt.scatter(range(len(err)), err)\n",
        "plt.scatter(range(len(acc)), acc)\n",
        "plt.title('model loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['loss','acc'], loc = 'lower left')\n",
        "plt.show()\n",
        "print(\"\\ntraining accurarcy: \", acc[-1])\n",
        "\n",
        "\"\"\"\n",
        "********************************** Testing **********************************\n",
        "\"\"\"\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "layer1.forward(X_test)                      \n",
        "activation1.forward(layer1.output)                        \n",
        "layer2.forward(layer1.output)\n",
        "activation2.forward(layer2.output) \n",
        "layer3.forward(activation2.output)\n",
        "#output_layer.forward(layer3.output, Y_test)\n",
        "#test_predictions = np.argmax(output_layer.activation.output, axis = 1)\n",
        "#print(test_predictions)\n",
        "#Output layer\n",
        "exp_values = np.exp(layer3.output - np.max(layer3.output, axis = 1, keepdims = True))\n",
        "probabilities = exp_values/np.sum(exp_values, axis = 1, keepdims = True)\n",
        "\n",
        "test_predictions = np.argmax(probabilities, axis = 1)\n",
        "\n",
        "test_accuracy = np.mean(test_predictions == Y_test)\n",
        "\n",
        "print('test_predictions: ', test_predictions)\n",
        "print('test_labels: ', Y_test)\n",
        "\n",
        "print('test data accuracy: ',test_accuracy)\n"
      ]
    }
  ]
}